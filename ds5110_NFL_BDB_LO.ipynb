{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 5110 Final Project\n",
    "## 2020 NFL Big Data Bowl\n",
    "### Group 3: Anoop Nath, Lauren O'Donnell, OC Ofoma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Our final project is completing the [NFL Big Data Bowl](https://www.kaggle.com/competitions/nfl-big-data-bowl-2020/overview) prompt from 2020. This competition was hosted on Kaggle and asks the participants to compute the Continuous Ranked Probability Score (CRPS) for each play. The CRPS indicates the probability that a team gains that many yards on the play. The CRPS is computed by: \n",
    "\n",
    "$C = \\dfrac{1}{1199} \\sum^N_{m=1} \\sum^{99}_{n=-99} (P(y \\leq n) H(n-Y_m))^2 $\n",
    "\n",
    "where P is the predicted distribution, $N$ is the number of plays in the test set, $Y$ is the actual yardage and $H(x)$ is the Heaviside step function $(H(x) = 1$ for $x \\geq 0$ and zero otherwise).\n",
    "\n",
    "The submission will not score if any of the predicted values has $P(y \\leq k) \\ge P(y \\leq k + 1)$ for any $k$ (i.e. the CDF must be non-decreasing).\n",
    "\n",
    "As part of the DS 5110 project requirements, we are to generate three models utilizing techniques from this course in `pyspark`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset\n",
    "The data were provided as part of the [2020 NFL Big Data Bowl](https://www.kaggle.com/competitions/nfl-big-data-bowl-2020/) hosted on Kaggle. For this project, the data were downloaded from Kaggle locally and uploaded to a shared location. \n",
    "\n",
    "We utilized the resources on Rivanna to execute our project. Our data was stored and shared on Rivanna for us to use in one singular location. Code was shared on our [GitHub Repository](https://github.com/lauren-odonnell/DS5110_NFL_BigDataBowl).\n",
    "\n",
    "A data dictionary was provided by the 2020 NL Big Data Bowl hosted on Kaggle and used as a schema to import the data. The variables are described below (**cite**)\n",
    "\n",
    "* `GameId` - a unique game identifier\n",
    "* `PlayId` - a unique play identifier\n",
    "* `Team` - home or away\n",
    "* `X` - player position along the long axis of the field. See figure below.\n",
    "* `Y` - player position along the short axis of the field. See figure below.\n",
    "* `S` - speed in yards/second\n",
    "* `A` - acceleration in yards/second^2\n",
    "* `Dis` - distance traveled from prior time point, in yards\n",
    "* `Orientation` - orientation of player (deg)\n",
    "* `Dir` - angle of player motion (deg)\n",
    "* `NflId` - a unique identifier of the player\n",
    "* `DisplayName` - player's name\n",
    "* `JerseyNumber` - jersey number\n",
    "* `Season` - year of the season\n",
    "* `YardLine` - the yard line of the line of scrimmage\n",
    "* `Quarter` - game quarter (1-5, 5 == overtime)\n",
    "* `GameClock` - time on the game clock\n",
    "* `PossessionTeam` - team with possession\n",
    "* `Down` - the down (1-4)\n",
    "* `Distance` - yards needed for a first down\n",
    "* `FieldPosition` - which side of the field the play is happening on\n",
    "* `HomeScoreBeforePlay` - home team score before play started\n",
    "* `VisitorScoreBeforePlay` - visitor team score before play started\n",
    "* `NflIdRusher` - the NflId of the rushing player\n",
    "* `OffenseFormation` - offense formation\n",
    "* `OffensePersonnel` - offensive team positional grouping\n",
    "* `DefendersInTheBox` - number of defenders lined up near the line of scrimmage, spanning the width of the offensive line\n",
    "* `DefensePersonnel` - defensive team positional grouping\n",
    "* `PlayDirection` - direction the play is headed\n",
    "* `TimeHandoff` - UTC time of the handoff\n",
    "* `TimeSnap` - UTC time of the snap\n",
    "* `Yards` - the yardage gained on the play (you are predicting this)\n",
    "* `PlayerHeight` - player height (ft-in)\n",
    "* `PlayerWeight` - player weight (lbs)\n",
    "* `PlayerBirthDate` - birth date (mm/dd/yyyy)\n",
    "* `PlayerCollegeName` - where the player attended college\n",
    "* `Position` - the player's position (the specific role on the field that they typically play)\n",
    "* `HomeTeamAbbr` - home team abbreviation\n",
    "* `VisitorTeamAbbr` - visitor team abbreviation\n",
    "* `Week` - week into the season\n",
    "* `Stadium` - stadium where the game is being played\n",
    "* `Location` - city where the game is being played\n",
    "* `StadiumType` - description of the stadium environment\n",
    "* `Turf` - description of the field surface\n",
    "* `GameWeather` - description of the game weather\n",
    "* `Temperature` - temperature (deg F)\n",
    "* `Humidity` - humidity\n",
    "* `WindSpeed` - wind speed in miles/hour\n",
    "* `WindDirection` - wind direction* \n",
    "\n",
    "\n",
    "<img src=\"NFL_Data_Dictionary.png\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sfs/qumulo/qhome/qsq6zz/ds5110/project\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/28 16:32:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"comm\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting dataset schema for import\n",
    "\n",
    "# import data types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('GameID', IntegerType(), True),\n",
    "    StructField('PlayID', IntegerType(), True), \n",
    "    StructField('Team', StringType(), True), \n",
    "    StructField('X', FloatType(), True), \n",
    "    StructField('Y', FloatType(), True), \n",
    "    StructField('S', FloatType(), True), \n",
    "    StructField('A', FloatType(), True), \n",
    "    StructField('Dis', FloatType(), True), \n",
    "    StructField('Orientation', FloatType(), True), \n",
    "    StructField('Dir', FloatType(), True), \n",
    "    StructField('NflId', IntegerType(), True), \n",
    "    StructField('DisplayName', StringType(), True), \n",
    "    StructField('JerseyNumber', IntegerType(), True), \n",
    "    StructField('Season', IntegerType(), True), \n",
    "    StructField('YardLine', IntegerType(), True), \n",
    "    StructField('Quarter', IntegerType(), True), \n",
    "    StructField('GameClock', TimestampType(), True), \n",
    "    StructField('Possessionteam', StringType(), True), \n",
    "    StructField('Down', IntegerType(), True), \n",
    "    StructField('Distance', IntegerType(), True), \n",
    "    StructField('FieldPosition', StringType(), True), \n",
    "    StructField('HomeScoreBeforePlay', IntegerType(), True), \n",
    "    StructField('VisitorScoreBeforePlay', IntegerType(), True), \n",
    "    StructField('NflIdRusher', IntegerType(), True), \n",
    "    StructField('OffenseFormation', StringType(), True), \n",
    "    StructField('OffensePersonnel', StringType(), True), \n",
    "    StructField('DefendersInTheBox', IntegerType(), True), \n",
    "    StructField('DefensePersonnel', StringType(), True), \n",
    "    StructField('PlayDirection', StringType(), True), \n",
    "    StructField('TimeHandoff', TimestampType(), True), \n",
    "    StructField('TimeSnap', TimestampType(), True), \n",
    "    StructField('Yards', IntegerType(), True), \n",
    "    StructField('PlayerHeight', StringType(), True), \n",
    "    StructField('PlayerWeight', IntegerType(), True), \n",
    "    StructField('PlayerBirthDate', StringType(), True), \n",
    "    StructField('PlayerCollegeName', StringType(), True), \n",
    "    StructField('Position', StringType(), True), \n",
    "    StructField('HomeTeamAbbr', StringType(), True), \n",
    "    StructField('VisitorTeamAbbr', StringType(), True), \n",
    "    StructField('Week', IntegerType(), True), \n",
    "    StructField('Stadium', StringType(), True), \n",
    "    StructField('Location', StringType(), True), \n",
    "    StructField('StadiumType', StringType(), True), \n",
    "    StructField('Turf', StringType(), True), \n",
    "    StructField('GameWeather', StringType(), True), \n",
    "    StructField('Temperature', IntegerType(), True), \n",
    "    StructField('Humidity', IntegerType(), True), \n",
    "    StructField('WindSpeed', IntegerType(), True), \n",
    "    StructField('WindDirection', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/sfs/qumulo/qhome/nux9aq/project' # update as necessary; current is the shared location containing the data\n",
    "\n",
    "data = spark.read.csv(path, header = True, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/28 16:32:42 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+-----+-----+----+----+---+-----------+------+------+-----------+------------+------+--------+-------+-------------------+--------------+----+--------+-------------+-------------------+----------------------+-----------+----------------+----------------+-----------------+----------------+-------------+-------------------+-------------------+-----+------------+------------+---------------+-----------------+--------+------------+---------------+----+----------------+--------------+-----------+----------+--------------+-----------+--------+---------+-------------+\n",
      "|    GameID|PlayID|Team|    X|    Y|   S|   A|Dis|Orientation|   Dir| NflId|DisplayName|JerseyNumber|Season|YardLine|Quarter|          GameClock|Possessionteam|Down|Distance|FieldPosition|HomeScoreBeforePlay|VisitorScoreBeforePlay|NflIdRusher|OffenseFormation|OffensePersonnel|DefendersInTheBox|DefensePersonnel|PlayDirection|        TimeHandoff|           TimeSnap|Yards|PlayerHeight|PlayerWeight|PlayerBirthDate|PlayerCollegeName|Position|HomeTeamAbbr|VisitorTeamAbbr|Week|         Stadium|      Location|StadiumType|      Turf|   GameWeather|Temperature|Humidity|WindSpeed|WindDirection|\n",
      "+----------+------+----+-----+-----+----+----+---+-----------+------+------+-----------+------------+------+--------+-------+-------------------+--------------+----+--------+-------------+-------------------+----------------------+-----------+----------------+----------------+-----------------+----------------+-------------+-------------------+-------------------+-----+------------+------------+---------------+-----------------+--------+------------+---------------+----+----------------+--------------+-----------+----------+--------------+-----------+--------+---------+-------------+\n",
      "|2017090700|  null|away|73.91|34.84|1.69|1.13|0.4|      81.99|177.18|496723| Eric Berry|          29|  2017|      35|      1|2023-07-28 14:14:00|            NE|   3|       2|           NE|                  0|                     0|    2543773|         SHOTGUN|1 RB, 1 TE, 3 WR|                6|2 DL, 3 LB, 6 DB|         left|2017-09-08 00:44:06|2017-09-08 00:44:05|    8|         6-0|         212|     12/29/1988|        Tennessee|      SS|          NE|             KC|   1|Gillette Stadium|Foxborough, MA|    Outdoor|Field Turf|Clear and warm|         63|      77|        8|           SW|\n",
      "+----------+------+----+-----+-----+----+----+---+-----------+------+------+-----------+------------+------+--------+-------+-------------------+--------------+----+--------+-------------+-------------------+----------------------+-----------+----------------+----------------+-----------------+----------------+-------------+-------------------+-------------------+-----+------------+------------+---------------+-----------------+--------+------------+---------------+----+----------------+--------------+-----------+----------+--------------+-----------+--------+---------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  682801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('Number of rows: ', data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "The NFL data is notorious for requiring lots of wrangling, from cleaning up sensors that were worn backwards during a game to engineering variables to fit the need of the research question at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting/Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n",
    "**NOTES:** Need at least two graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Construction\n",
    "**NOTES:** Ideally created with a pipeline and will include the following: \n",
    "\n",
    "    a. A benchmark model, which is relatively simple. This could be a regression model with a small number of features (possibly a single feature). This provides a basis for comparison and a sanity check.\n",
    "    b. Two relatively more sophisticated models (e.g., random forest, gradient boosted tree). The best model found in your experiments is called the champion model. The model construction process should follow the best practices covered in class, including:\n",
    "    a. Data preprocessing. The required steps will depend on the model, and could include:\n",
    "        i. dummy variable construction\n",
    "        ii. feature scaling\n",
    "        iii. handling missing values and outliers\n",
    "        iv. handling semi-structured / unstructured data\n",
    "        v. dimensionality reduction (e.g., PCA)\n",
    "    b. Data splitting (train/validation/test sets, for example). The test set should be left out for evaluation purposes. It should NOT be used in training.\n",
    "    c. K-fold cross validation of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "**NOTES:** This should include computation of relevant metrics, and a comparison between models.\n",
    "\n",
    "For all appropriate models (benchmark, champion, and other relevant models), the following should be\n",
    "conducted:\n",
    "    \n",
    "    a. Evaluate relevant metrics. \n",
    "        For regression, this would include\n",
    "            i. R-squared\n",
    "        For classification, this would include:\n",
    "            i. accuracy\n",
    "            ii. precision, recall, F1 score\n",
    "            iii. confusion matrix\n",
    "            iv. area under ROC curve (AUROC)\n",
    "        Depending on the application, additional evaluation could make sense such as lift charts\n",
    "    b. Sensitivity analysis\n",
    "        For your champion model, show relevant metrics for different hyperparameter values. This gives an idea of the model sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS5110 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
